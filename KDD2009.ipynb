{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercício 1\n",
    "\n",
    "O KDD é, sem dúvida, uma das conferências de maior renome nas áreas acadêmica e industrial. Uma das tradicionais atividades propostas nesta conferência são as competições de aprendizagem de máquina que, em geral, utilizam dados reais disponibilizados por empresas interessadas nas soluções desenvolvidas por essa numericamente restrita, mas considerável comunidade acadêmica.\n",
    "\n",
    "O KDD cup de 2009 propõem um problema de classificação binária para 50k contas de cartão de crédito. Para tal, eles disponibilizaram dois datasets, um \"small\" e um \"large\", formados por 230 e 15k informações/atributos de cada uma dessas contas. O objetivo é predizer ações que os donos destas contas irão tomar considerando três possibilidades: cancelar uma conta, denominado churn; usar novos produtos/serviços, denominado appetency; e comprar upgrades, denominado (up-selling).\n",
    "\n",
    "Como solução para esse problema, dividi a tarefa de classificação/predição em 4 sub-tarefas que, ao meu ver, são igualmente relevantes para o desenvolvimento adequado da maioria dos problemas de classificação:\n",
    "\n",
    "1. Caracterização da coleção de dados;\n",
    "2. Preprocessamento e manipulação dos dados;\n",
    "3. Classificação;\n",
    "4. Avaliação dos resultados obtidos;\n",
    "\n",
    "Cada uma das sub-tarefas são apresentadas nas Seções sequentes e uma solução final é proposta considerando as melhores configurações experimentadas neste arquivo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise da Coleção de Dados\n",
    "\n",
    "Para propor soluções adequadas para cada problema, faz-se necessário compreender quais são as características principais da coleção de dados que se está usando. Para tanto, farei uso de ferramentas de análise de dados e manipulações matemáticas, como, por exemplo, DataFrames do pandas e o Numpy, para facilitar a implementação da solução.\n",
    "\n",
    "O pandas é um toolkit de abstração de estrutura de dados em matrizes indexadas. Em outras palavras, o pandas facilita manipulação por linha e/ou coluna de bases de dados estritamente grandes, disponibilizando funções eficiêntes para manipulações iterativas (e/ou paralelas) sob os dados.\n",
    "\n",
    "Além desses toolkits mencionais, uso sklearn e imbalanced-learn, especificamente, para subtarefas associadas a aprendizagem de máquina.\n",
    "\n",
    "### Aquisição da Coleção de Dados\n",
    "\n",
    "Farei uso da estrutura de dados DataFrame do pandas para importar os dados do problema em questão. Como os dados estão em formato tsv (separando cada atributo das instancias por tabulação), a importação é feita quase que de forma similar à chamada da função read_csv.\n",
    "\n",
    "Além disso, serão importados os dados relativos ao ground truth, ou seja, a resposta esperada para cada instância, considerando as três tarefas de predição possíveis: up-selling, appetency, e churn, denominados como y_upsell, y_appe e y_churn, respectivamente.\n",
    "\n",
    "Por questões de limite no armazenamento no github, os dados usados por esse arquivo não estão disponíveis a priori, mas podem ser baixados [aqui](https://drive.google.com/drive/folders/1oD__SautpLOYXpgV2S1S6Yw_i7XrIfND).\n",
    "\n",
    "Assim, primeiramente faça o donwload deles para o caminho \"data/small/\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X        = pd.read_csv('data/small/orange_small_train.data', sep='\\t')\n",
    "y_upsell = pd.read_csv('data/small/orange_small_train_upselling.labels', header=None)\n",
    "y_appe   = pd.read_csv('data/small/orange_small_train_appetency.labels', header=None)\n",
    "y_churn  = pd.read_csv('data/small/orange_small_train_churn.labels', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Características da Coleção de Dados\n",
    "\n",
    "Uma característica peculiar dessa coleção de dados é a grande quantidade de atributos ausentes ou nulos, representado nessa estrutuda como NaN (Not a Number). \n",
    "\n",
    "Inclusive, algumas colunas nunca sequer aperecem valores que não são NaN, como é o exemplo da coluna Var8 e Var230.\n",
    "\n",
    "Abaixo são apresentados as primeiras 5 instâncias (contas de cartão de crédito) da coleção de dados e seus atributos e a seguir a quantidade de NaN cada atributo tem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Var1</th>\n",
       "      <th>Var2</th>\n",
       "      <th>Var3</th>\n",
       "      <th>Var4</th>\n",
       "      <th>Var5</th>\n",
       "      <th>Var6</th>\n",
       "      <th>Var7</th>\n",
       "      <th>Var8</th>\n",
       "      <th>Var9</th>\n",
       "      <th>Var10</th>\n",
       "      <th>...</th>\n",
       "      <th>Var221</th>\n",
       "      <th>Var222</th>\n",
       "      <th>Var223</th>\n",
       "      <th>Var224</th>\n",
       "      <th>Var225</th>\n",
       "      <th>Var226</th>\n",
       "      <th>Var227</th>\n",
       "      <th>Var228</th>\n",
       "      <th>Var229</th>\n",
       "      <th>Var230</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1526.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>oslk</td>\n",
       "      <td>fXVEsaq</td>\n",
       "      <td>jySVZNlOJy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>xb3V</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>525.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>oslk</td>\n",
       "      <td>2Kb5FSF</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>fKCe</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5236.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Al6ZaUT</td>\n",
       "      <td>NKv4yOc</td>\n",
       "      <td>jySVZNlOJy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kG3k</td>\n",
       "      <td>Qu4f</td>\n",
       "      <td>02N6s8f</td>\n",
       "      <td>ib5G6X1eUxUn6</td>\n",
       "      <td>am7c</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>oslk</td>\n",
       "      <td>CE7uk3u</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1029.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>oslk</td>\n",
       "      <td>1J2cvxe</td>\n",
       "      <td>LM8l689qOp</td>\n",
       "      <td>NaN</td>\n",
       "      <td>kG3k</td>\n",
       "      <td>FSa2</td>\n",
       "      <td>RAYp</td>\n",
       "      <td>F2FyR07IdsN7I</td>\n",
       "      <td>mj86</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 230 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Var1  Var2  Var3  Var4  Var5    Var6  Var7  Var8  Var9  Var10   ...    \\\n",
       "0   NaN   NaN   NaN   NaN   NaN  1526.0   7.0   NaN   NaN    NaN   ...     \n",
       "1   NaN   NaN   NaN   NaN   NaN   525.0   0.0   NaN   NaN    NaN   ...     \n",
       "2   NaN   NaN   NaN   NaN   NaN  5236.0   7.0   NaN   NaN    NaN   ...     \n",
       "3   NaN   NaN   NaN   NaN   NaN     NaN   0.0   NaN   NaN    NaN   ...     \n",
       "4   NaN   NaN   NaN   NaN   NaN  1029.0   7.0   NaN   NaN    NaN   ...     \n",
       "\n",
       "    Var221   Var222      Var223  Var224  Var225  Var226   Var227  \\\n",
       "0     oslk  fXVEsaq  jySVZNlOJy     NaN     NaN    xb3V     RAYp   \n",
       "1     oslk  2Kb5FSF  LM8l689qOp     NaN     NaN    fKCe     RAYp   \n",
       "2  Al6ZaUT  NKv4yOc  jySVZNlOJy     NaN    kG3k    Qu4f  02N6s8f   \n",
       "3     oslk  CE7uk3u  LM8l689qOp     NaN     NaN    FSa2     RAYp   \n",
       "4     oslk  1J2cvxe  LM8l689qOp     NaN    kG3k    FSa2     RAYp   \n",
       "\n",
       "          Var228  Var229  Var230  \n",
       "0  F2FyR07IdsN7I     NaN     NaN  \n",
       "1  F2FyR07IdsN7I     NaN     NaN  \n",
       "2  ib5G6X1eUxUn6    am7c     NaN  \n",
       "3  F2FyR07IdsN7I     NaN     NaN  \n",
       "4  F2FyR07IdsN7I    mj86     NaN  \n",
       "\n",
       "[5 rows x 230 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Coluna Var1 tem 49298 NaN (98.60%)\n",
      " Coluna Var2 tem 48759 NaN (97.52%)\n",
      " Coluna Var3 tem 48760 NaN (97.52%)\n",
      " Coluna Var4 tem 48421 NaN (96.84%)\n",
      " Coluna Var5 tem 48513 NaN (97.03%)\n",
      " Coluna Var6 tem 5529 NaN (11.06%)\n",
      " Coluna Var7 tem 5539 NaN (11.08%)\n",
      " Coluna Var8 tem 50000 NaN (100.00%)\n",
      " Coluna Var9 tem 49298 NaN (98.60%)\n",
      " Coluna Var10 tem 48513 NaN (97.03%)\n",
      " Coluna Var11 tem 48760 NaN (97.52%)\n",
      " Coluna Var12 tem 49442 NaN (98.88%)\n",
      " Coluna Var13 tem 5539 NaN (11.08%)\n",
      " Coluna Var14 tem 48760 NaN (97.52%)\n",
      " Coluna Var15 tem 50000 NaN (100.00%)\n",
      " Coluna Var16 tem 48513 NaN (97.03%)\n",
      " Coluna Var17 tem 48421 NaN (96.84%)\n",
      " Coluna Var18 tem 48421 NaN (96.84%)\n",
      " Coluna Var19 tem 48421 NaN (96.84%)\n",
      " Coluna Var20 tem 50000 NaN (100.00%)\n",
      " Coluna Var21 tem 5529 NaN (11.06%)\n",
      " Coluna Var22 tem 5009 NaN (10.02%)\n",
      " Coluna Var23 tem 48513 NaN (97.03%)\n",
      " Coluna Var24 tem 7230 NaN (14.46%)\n",
      " Coluna Var25 tem 5009 NaN (10.02%)\n",
      " Coluna Var26 tem 48513 NaN (97.03%)\n",
      " Coluna Var27 tem 48513 NaN (97.03%)\n",
      " Coluna Var28 tem 5011 NaN (10.02%)\n",
      " Coluna Var29 tem 49298 NaN (98.60%)\n",
      " Coluna Var30 tem 49298 NaN (98.60%)\n",
      " Coluna Var31 tem 50000 NaN (100.00%)\n",
      " Coluna Var32 tem 50000 NaN (100.00%)\n",
      " Coluna Var33 tem 49153 NaN (98.31%)\n",
      " Coluna Var34 tem 48759 NaN (97.52%)\n",
      " Coluna Var35 tem 5009 NaN (10.02%)\n",
      " Coluna Var36 tem 48759 NaN (97.52%)\n",
      " Coluna Var37 tem 48421 NaN (96.84%)\n",
      " Coluna Var38 tem 5009 NaN (10.02%)\n",
      " Coluna Var39 tem 50000 NaN (100.00%)\n",
      " Coluna Var40 tem 48759 NaN (97.52%)\n",
      " Coluna Var41 tem 49298 NaN (98.60%)\n",
      " Coluna Var42 tem 50000 NaN (100.00%)\n",
      " Coluna Var43 tem 48759 NaN (97.52%)\n",
      " Coluna Var44 tem 5009 NaN (10.02%)\n",
      " Coluna Var45 tem 49656 NaN (99.31%)\n",
      " Coluna Var46 tem 48759 NaN (97.52%)\n",
      " Coluna Var47 tem 49298 NaN (98.60%)\n",
      " Coluna Var48 tem 50000 NaN (100.00%)\n",
      " Coluna Var49 tem 48759 NaN (97.52%)\n",
      " Coluna Var50 tem 49298 NaN (98.60%)\n",
      " Coluna Var51 tem 46253 NaN (92.51%)\n",
      " Coluna Var52 tem 50000 NaN (100.00%)\n",
      " Coluna Var53 tem 49298 NaN (98.60%)\n",
      " Coluna Var54 tem 48759 NaN (97.52%)\n",
      " Coluna Var55 tem 50000 NaN (100.00%)\n",
      " Coluna Var56 tem 49354 NaN (98.71%)\n",
      " Coluna Var57 tem 0 NaN (0.00%)\n",
      " Coluna Var58 tem 49298 NaN (98.60%)\n",
      " Coluna Var59 tem 49180 NaN (98.36%)\n",
      " Coluna Var60 tem 48513 NaN (97.03%)\n",
      " Coluna Var61 tem 49153 NaN (98.31%)\n",
      " Coluna Var62 tem 49442 NaN (98.88%)\n",
      " Coluna Var63 tem 49306 NaN (98.61%)\n",
      " Coluna Var64 tem 49762 NaN (99.52%)\n",
      " Coluna Var65 tem 5539 NaN (11.08%)\n",
      " Coluna Var66 tem 49306 NaN (98.61%)\n",
      " Coluna Var67 tem 48513 NaN (97.03%)\n",
      " Coluna Var68 tem 48759 NaN (97.52%)\n",
      " Coluna Var69 tem 48513 NaN (97.03%)\n",
      " Coluna Var70 tem 48513 NaN (97.03%)\n",
      " Coluna Var71 tem 48871 NaN (97.74%)\n",
      " Coluna Var72 tem 22380 NaN (44.76%)\n",
      " Coluna Var73 tem 0 NaN (0.00%)\n",
      " Coluna Var74 tem 5539 NaN (11.08%)\n",
      " Coluna Var75 tem 48759 NaN (97.52%)\n",
      " Coluna Var76 tem 5009 NaN (10.02%)\n",
      " Coluna Var77 tem 49298 NaN (98.60%)\n",
      " Coluna Var78 tem 5009 NaN (10.02%)\n",
      " Coluna Var79 tem 50000 NaN (100.00%)\n",
      " Coluna Var80 tem 48513 NaN (97.03%)\n",
      " Coluna Var81 tem 5529 NaN (11.06%)\n",
      " Coluna Var82 tem 48421 NaN (96.84%)\n",
      " Coluna Var83 tem 5009 NaN (10.02%)\n",
      " Coluna Var84 tem 48760 NaN (97.52%)\n",
      " Coluna Var85 tem 5009 NaN (10.02%)\n",
      " Coluna Var86 tem 49298 NaN (98.60%)\n",
      " Coluna Var87 tem 49298 NaN (98.60%)\n",
      " Coluna Var88 tem 48917 NaN (97.83%)\n",
      " Coluna Var89 tem 49354 NaN (98.71%)\n",
      " Coluna Var90 tem 49298 NaN (98.60%)\n",
      " Coluna Var91 tem 48871 NaN (97.74%)\n",
      " Coluna Var92 tem 49829 NaN (99.66%)\n",
      " Coluna Var93 tem 48513 NaN (97.03%)\n",
      " Coluna Var94 tem 22380 NaN (44.76%)\n",
      " Coluna Var95 tem 48759 NaN (97.52%)\n",
      " Coluna Var96 tem 48759 NaN (97.52%)\n",
      " Coluna Var97 tem 48513 NaN (97.03%)\n",
      " Coluna Var98 tem 49442 NaN (98.88%)\n",
      " Coluna Var99 tem 48421 NaN (96.84%)\n",
      " Coluna Var100 tem 49298 NaN (98.60%)\n",
      " Coluna Var101 tem 49127 NaN (98.25%)\n",
      " Coluna Var102 tem 49549 NaN (99.10%)\n",
      " Coluna Var103 tem 48513 NaN (97.03%)\n",
      " Coluna Var104 tem 49180 NaN (98.36%)\n",
      " Coluna Var105 tem 49180 NaN (98.36%)\n",
      " Coluna Var106 tem 48421 NaN (96.84%)\n",
      " Coluna Var107 tem 48513 NaN (97.03%)\n",
      " Coluna Var108 tem 49298 NaN (98.60%)\n",
      " Coluna Var109 tem 7230 NaN (14.46%)\n",
      " Coluna Var110 tem 49298 NaN (98.60%)\n",
      " Coluna Var111 tem 48871 NaN (97.74%)\n",
      " Coluna Var112 tem 5009 NaN (10.02%)\n",
      " Coluna Var113 tem 0 NaN (0.00%)\n",
      " Coluna Var114 tem 48759 NaN (97.52%)\n",
      " Coluna Var115 tem 49180 NaN (98.36%)\n",
      " Coluna Var116 tem 49298 NaN (98.60%)\n",
      " Coluna Var117 tem 48421 NaN (96.84%)\n",
      " Coluna Var118 tem 49829 NaN (99.66%)\n",
      " Coluna Var119 tem 5529 NaN (11.06%)\n",
      " Coluna Var120 tem 48513 NaN (97.03%)\n",
      " Coluna Var121 tem 49298 NaN (98.60%)\n",
      " Coluna Var122 tem 48759 NaN (97.52%)\n",
      " Coluna Var123 tem 5009 NaN (10.02%)\n",
      " Coluna Var124 tem 48421 NaN (96.84%)\n",
      " Coluna Var125 tem 5539 NaN (11.08%)\n",
      " Coluna Var126 tem 13920 NaN (27.84%)\n",
      " Coluna Var127 tem 48917 NaN (97.83%)\n",
      " Coluna Var128 tem 48917 NaN (97.83%)\n",
      " Coluna Var129 tem 49298 NaN (98.60%)\n",
      " Coluna Var130 tem 48760 NaN (97.52%)\n",
      " Coluna Var131 tem 49298 NaN (98.60%)\n",
      " Coluna Var132 tem 5009 NaN (10.02%)\n",
      " Coluna Var133 tem 5009 NaN (10.02%)\n",
      " Coluna Var134 tem 5009 NaN (10.02%)\n",
      " Coluna Var135 tem 48421 NaN (96.84%)\n",
      " Coluna Var136 tem 49306 NaN (98.61%)\n",
      " Coluna Var137 tem 49298 NaN (98.60%)\n",
      " Coluna Var138 tem 48421 NaN (96.84%)\n",
      " Coluna Var139 tem 48513 NaN (97.03%)\n",
      " Coluna Var140 tem 5539 NaN (11.08%)\n",
      " Coluna Var141 tem 50000 NaN (100.00%)\n",
      " Coluna Var142 tem 49298 NaN (98.60%)\n",
      " Coluna Var143 tem 5009 NaN (10.02%)\n",
      " Coluna Var144 tem 5529 NaN (11.06%)\n",
      " Coluna Var145 tem 48421 NaN (96.84%)\n",
      " Coluna Var146 tem 48513 NaN (97.03%)\n",
      " Coluna Var147 tem 48513 NaN (97.03%)\n",
      " Coluna Var148 tem 48513 NaN (97.03%)\n",
      " Coluna Var149 tem 7230 NaN (14.46%)\n",
      " Coluna Var150 tem 48421 NaN (96.84%)\n",
      " Coluna Var151 tem 49153 NaN (98.31%)\n",
      " Coluna Var152 tem 48421 NaN (96.84%)\n",
      " Coluna Var153 tem 5009 NaN (10.02%)\n",
      " Coluna Var154 tem 49298 NaN (98.60%)\n",
      " Coluna Var155 tem 48421 NaN (96.84%)\n",
      " Coluna Var156 tem 49306 NaN (98.61%)\n",
      " Coluna Var157 tem 48871 NaN (97.74%)\n",
      " Coluna Var158 tem 49127 NaN (98.25%)\n",
      " Coluna Var159 tem 48759 NaN (97.52%)\n",
      " Coluna Var160 tem 5009 NaN (10.02%)\n",
      " Coluna Var161 tem 48421 NaN (96.84%)\n",
      " Coluna Var162 tem 48759 NaN (97.52%)\n",
      " Coluna Var163 tem 5009 NaN (10.02%)\n",
      " Coluna Var164 tem 48421 NaN (96.84%)\n",
      " Coluna Var165 tem 49127 NaN (98.25%)\n",
      " Coluna Var166 tem 48513 NaN (97.03%)\n",
      " Coluna Var167 tem 50000 NaN (100.00%)\n",
      " Coluna Var168 tem 49298 NaN (98.60%)\n",
      " Coluna Var169 tem 50000 NaN (100.00%)\n",
      " Coluna Var170 tem 48759 NaN (97.52%)\n",
      " Coluna Var171 tem 48917 NaN (97.83%)\n",
      " Coluna Var172 tem 48513 NaN (97.03%)\n",
      " Coluna Var173 tem 5009 NaN (10.02%)\n",
      " Coluna Var174 tem 48421 NaN (96.84%)\n",
      " Coluna Var175 tem 50000 NaN (100.00%)\n",
      " Coluna Var176 tem 48760 NaN (97.52%)\n",
      " Coluna Var177 tem 48759 NaN (97.52%)\n",
      " Coluna Var178 tem 49354 NaN (98.71%)\n",
      " Coluna Var179 tem 48421 NaN (96.84%)\n",
      " Coluna Var180 tem 49298 NaN (98.60%)\n",
      " Coluna Var181 tem 5009 NaN (10.02%)\n",
      " Coluna Var182 tem 48421 NaN (96.84%)\n",
      " Coluna Var183 tem 48759 NaN (97.52%)\n",
      " Coluna Var184 tem 48759 NaN (97.52%)\n",
      " Coluna Var185 tem 50000 NaN (100.00%)\n",
      " Coluna Var186 tem 49298 NaN (98.60%)\n",
      " Coluna Var187 tem 49298 NaN (98.60%)\n",
      " Coluna Var188 tem 48759 NaN (97.52%)\n",
      " Coluna Var189 tem 28978 NaN (57.96%)\n",
      " Coluna Var190 tem 49667 NaN (99.33%)\n",
      " Coluna Var191 tem 48917 NaN (97.83%)\n",
      " Coluna Var192 tem 369 NaN (0.74%)\n",
      " Coluna Var193 tem 0 NaN (0.00%)\n",
      " Coluna Var194 tem 37216 NaN (74.43%)\n",
      " Coluna Var195 tem 0 NaN (0.00%)\n",
      " Coluna Var196 tem 0 NaN (0.00%)\n",
      " Coluna Var197 tem 143 NaN (0.29%)\n",
      " Coluna Var198 tem 0 NaN (0.00%)\n",
      " Coluna Var199 tem 4 NaN (0.01%)\n",
      " Coluna Var200 tem 25408 NaN (50.82%)\n",
      " Coluna Var201 tem 37217 NaN (74.43%)\n",
      " Coluna Var202 tem 1 NaN (0.00%)\n",
      " Coluna Var203 tem 143 NaN (0.29%)\n",
      " Coluna Var204 tem 0 NaN (0.00%)\n",
      " Coluna Var205 tem 1934 NaN (3.87%)\n",
      " Coluna Var206 tem 5529 NaN (11.06%)\n",
      " Coluna Var207 tem 0 NaN (0.00%)\n",
      " Coluna Var208 tem 143 NaN (0.29%)\n",
      " Coluna Var209 tem 50000 NaN (100.00%)\n",
      " Coluna Var210 tem 0 NaN (0.00%)\n",
      " Coluna Var211 tem 0 NaN (0.00%)\n",
      " Coluna Var212 tem 0 NaN (0.00%)\n",
      " Coluna Var213 tem 48871 NaN (97.74%)\n",
      " Coluna Var214 tem 25408 NaN (50.82%)\n",
      " Coluna Var215 tem 49306 NaN (98.61%)\n",
      " Coluna Var216 tem 0 NaN (0.00%)\n",
      " Coluna Var217 tem 703 NaN (1.41%)\n",
      " Coluna Var218 tem 703 NaN (1.41%)\n",
      " Coluna Var219 tem 5211 NaN (10.42%)\n",
      " Coluna Var220 tem 0 NaN (0.00%)\n",
      " Coluna Var221 tem 0 NaN (0.00%)\n",
      " Coluna Var222 tem 0 NaN (0.00%)\n",
      " Coluna Var223 tem 5211 NaN (10.42%)\n",
      " Coluna Var224 tem 49180 NaN (98.36%)\n",
      " Coluna Var225 tem 26144 NaN (52.29%)\n",
      " Coluna Var226 tem 0 NaN (0.00%)\n",
      " Coluna Var227 tem 0 NaN (0.00%)\n",
      " Coluna Var228 tem 0 NaN (0.00%)\n",
      " Coluna Var229 tem 28432 NaN (56.86%)\n",
      " Coluna Var230 tem 50000 NaN (100.00%)\n"
     ]
    }
   ],
   "source": [
    "for col in X:\n",
    "    n_nan = X[col].isnull().sum()\n",
    "    print(\" Coluna %s tem %d NaN (%.2f%%)\" % (col, n_nan, 100.*n_nan/len(X[col])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outra importante caracterização que se pode fazer sob os dados é o tipo que eles são representados. Como descrito na apresentação dos dados do problema na página do [KDD Cup 2009](http://www.kdd.org/kdd-cup/view/kdd-cup-2009/Data), essa versão small dos dados consiste de 190 atributos numéricos (com os mais variados intervalos) e 40 atributos categóricos.\n",
    "\n",
    "Considerando que não sabemos o significado de cada um dos atributos, vou selecionar o atributo \"Var16\" casos para exemplificar a distribuição."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f0be78a3940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGfSS1Lj/DwgdCKngxP3OAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0bedb7e780>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "X['Var16'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_columns = ['Var%d' % d for d in range(1,191)]   # Nome dos atributos numéricos\n",
    "cat_columns = ['Var%d' % d for d in range(191,231)] # Nome dos atributos categóricos \n",
    "X[num_columns] = X[num_columns].astype(np.float64)  # Atributos numéricos\n",
    "X[cat_columns] = X[cat_columns].astype(np.str)      # Atributos categóricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como já mencionado, a coleção de dados consiste, então, de 230 atributos, sendo 40 categóricos e 190 numéricos, podendo conter, para algumas instâncias valores ausententes de atributos, viéses e, até mesmo, atributos completamentes ausentes em todas as instâncias. Trata-se, portanto, de uma coleção de dados esparsa, sendo necessário uso de manipulação durante o preprocessamento para adequar os dados às diferentes abordagens de classificação.\n",
    "\n",
    "Além disso, as classes a serem preditas são também desbalanceadas, havendo uma discrepancia grande entre a quantidade de instâncias positivas da quantidade de instâncias negativas na coleção. Isso fica evidênte ao observarmos o histograma das frequências de cada uma das ações possíveis no problema, que é apresentado abaixo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5,0.98,'Up-selling')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEVCAYAAAALsCk2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqcdwkCT1/D/e0mr4nn3+8AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0be78ba5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEVCAYAAAALsCk2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0be250c668>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEVCAYAAAALsCk2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6vx/DCItQyTKzWEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0bddc606d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pylab as pl\n",
    "\n",
    "y_appe.hist()\n",
    "pl.suptitle('Appetency')\n",
    "\n",
    "y_churn.hist()\n",
    "pl.suptitle('Churn')\n",
    "\n",
    "y_upsell.hist()\n",
    "pl.suptitle('Up-selling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessamento\n",
    "\n",
    "Como visto, a coleção de dados usada está, consideravelmente, não padronizada. Nessa fase, portanto, faz-se uso de funções de transformações dos dados, adenquando-os aos classificadores usados.\n",
    "\n",
    "### Encoder\n",
    "\n",
    "O encoder é responsável por converter valores de atributos categóricos para o formato aceito pelos classificadores usados para essa solução: atributos numérico. Aja vista que alguns classificadores permitem predição (ou até mesmo são restritos a predição) de instâncias com atributos categóricos, vou, por questões de restrição do número de combinações, converter os atributos categóricos desta base de dados para numéricos usando LabelEncoder(), restringindo, assim, o número de algoritmos de classificação a se experimentar.\n",
    "\n",
    "O LabelEncoder é um tipo de tranformação que se aplica sob os dados para converter as n categorias encontradas na série de instâncias em n-1 valores numéricos. Dessa forma, essa transformação mapeia cada um das N possiveis categorias (que, aqui incluem NaN como uma possibilidade) em um valor inteiro entre 0 e N-1.\n",
    "\n",
    "Como as implementações default de Encoding consideram como argumento apenas uma serie de valores categóricos e adiante farei uso de Pipelines para otimizar a execução da solução proposta, faço uso de uma implementação de encoding que leva em consideração um DataFrame como entrada. Essa implementação facilita também o encoding de conjuntos restritos de atributos a serem codificados, podendo, por exemplo, mapear apenas um subconjunto dos atributos do DataFrame, que é exatamente o caso que estamos tratando, onde apenas os ultimos 40 atritubos são categóricos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MultiColumnLabelEncoder:\n",
    "    def __init__(self,columns = None):\n",
    "        self.columns = columns # array of column names to encode\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self # not relevant here\n",
    "\n",
    "    def transform(self,X):\n",
    "        '''\n",
    "        Transforms columns of X specified in self.columns using\n",
    "        LabelEncoder(). If no columns specified, transforms all\n",
    "        columns in X.\n",
    "        '''\n",
    "        output = X.copy()\n",
    "        if self.columns is not None:\n",
    "            for col in self.columns:\n",
    "                output[col] = LabelEncoder().fit_transform(output[col])\n",
    "        else:\n",
    "            for colname,col in output.iteritems():\n",
    "                output[colname] = LabelEncoder().fit_transform(col)\n",
    "        return output\n",
    "\n",
    "    def fit_transform(self,X,y=None):\n",
    "        return self.fit(X,y).transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "multi_encoder = MultiColumnLabelEncoder(columns=cat_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputer\n",
    "\n",
    "O imputer é outra transformação relevante para solução, uma vez que converte os valores não numéricos restantes (os NaN) em valores interpretáveis pelos classificadores usados. Esse método permite que se escolha entre diferentes estratégias de conversão para valores numéricos. Entre as possiveis abordagens de conversão têm a média, mediana e a moda dos dados.\n",
    "\n",
    "Além disso, pode-se configurar essa estratégia para executar sob todos os atributos de uma mesma instância ou em todos os valores de um mesmo atributo na base de dados, sendo que serão considerados apenas os valores que são numéricos.\n",
    "\n",
    "Para essa transformação, fiz a escolha de converter valores NaN pela média dos atributos numéricos da instância, uma vez que, como mencionado anteriormente, alguns atributos não possuem sequer um valor numérico (como é o exemplo do atributo Var8).\n",
    "\n",
    "Poderia primeiramente remover esses atributos que são baseados apenas por NaNs. Contudo, de qualquer forma, imagino que a normalização pela média da instância faz mais sentido, uma vez que isso aumentaria a variância dos valores para cada um dos atributos e que, mesmo podendo ser um atributo irrelevante para a classificação, posteriormente será executada uma seleção de atributos que removerá, caso necessário, tais atributos irrelevantes para a classificação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Imputer, MinMaxScaler, RobustScaler\n",
    "imputer = Imputer(missing_values=np.NAN, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Escalares\n",
    "\n",
    "A maioria dos algoritmos de classificação funcionam melhor se a escala dos valores numéricos for mais comportada. Ou seja, espera-se que os valores estejam com intervalos bem definidos ou com distribuição mais discrepante em relação as predições esperadas. Assim, essa tarefa visa transformar os valores numéricos com intervalos muito discriminantes em intervalos comportados, como, por exemplo, com valores entre 0 e 1.\n",
    "\n",
    "Dessa forma, faremos uso de duas transformações sob os dados: MinMax e Robust. A primeira converte os valores dos atributos para o intervalo [0,1] dividindo os valores pelo maior valor encontrado na transformação. Obviamente valores fora desse intervalo esperado podem ser encontrados na fase de teste, assim, aplicamos a outra transformação sob os dados, que visa normalizar tais valores considerados outliers e distribuir, de maneira mais uniforme, os valores dos atributos na coleção. Outiliers são valores atipicos encontrados na coleção de dados. Esses valores podem prejudicar a predição, uma vez que podem confundir a modelagem da função objetivo, ou até mesmo, na predição das instâncias de teste.\n",
    "\n",
    "Essa transformação em questão utiliza o IQR (ou, Interquartile Range do 1º e 3º quartil) que é uma estratégia de determinação de intervalos menos dependente de outliers, uma vez que as estratégias usuais levam em consideração a média e/ou o desvio padrão."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "minmax_scalar = MinMaxScaler()\n",
    "robust_scalar = RobustScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validação Cruzada com Amostragem Estratificada\n",
    "\n",
    "Um dos problemas mais tradicionais de abordagens de aprendizagem de máquina é o overfitting (ou, da terrível tradução, sobreajuste). O overfitting acontece quando o algoritmo de aprendizagem de máquina ajusta o modelo treinado, de forma tão consistente, que se mostra ineficaz na predição de instâncias não usadas na fase de treino.\n",
    "\n",
    "RandomForest é um exemplo de abordagem que, se não manipulado com a devida cautela, pode sobreajustar aos dados, tornando-o inviável na predição de novas instâncias.\n",
    "\n",
    "Assim, uma solução para se obter resultados mais confiáveis das predições é por meio do uso de validação cruzada para o treino e, até mesmo, avaliação de modelos. A validação cruzada consiste de dividir os dados de treino em K partes (denomiandas folds), onde serão usados K-1 partes dos dados para o treino do modelo e a parte sobressalente para o teste. O resultado final são K modelos e K resultados que descrevem, com maior confiança e menor variancia, o resultado esperado para os dados em questão.\n",
    "\n",
    "Ademais, validação cruzada pode ser um grande problema quando não levamos em consideração o viés do valor esperado. Por exemplo, para um determinado fold, podemos ter uma quantidade muito pequena de instâncias positivas, prejudicando o treino do modelo, ou até mesmo, a avaliação adequada do resultado.\n",
    "\n",
    "Assim, usei, para o treino e também para a avaliação, a amostragem estratificada (StratifiedKFold), que visa dividir igualmente a quantidade de instâncias poositivas entre os folds. Este algoritmo de validação cruzada tende a beneficiar também as fases de seleção de atributos e de over samplings, que serão descritas adiante.\n",
    "\n",
    "Para garantir uma replicabilidade dos resultados, que são definidos por abordagens estatísticas, fixamos o estado inicial dos métodos que fazem uso de inicialização aleatória para 42 (que é o significado da vida, do universo e tudo mais)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "cv = StratifiedKFold(n_splits=5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seleção de Atributos\n",
    "\n",
    "Como demonstrei anteriormente, considerando os valores dos atributos para essa tarefa, é esperável que alguns atributos sejam irrelevantes para auxiliar na predição das instâncias. Esse tipo de seleção é usada tanto para otimizar a eficiência do algoritmo de machine learning, quanto para otimizar a sua acurácia.\n",
    "\n",
    "Para tanto, escolhi o método de seleção de atributos que estima a importancia para cada atributo após o treino dos dados com classificadores fracos, sendo que o classificador \"fraco\" selecionado para essa tarefa é o SVC linear (Linear Support Vector Classification), que é o algoritmo de classificação que usa SVM (Support Vector Machines) com um kernel linear.\n",
    "\n",
    "Esse classificador que é usado para predizer a importância dos atributos, estima uma linha que tenta otimizar a separação entre as instâncias positivas das negativas usando os valores do n-atributos. Assim, ele quantifica a importância de cada atributo segundo o erro que este atributo retorna considerando o valor esperado da predição. Nessas condições, quanto menor for o valor do hiper-parâmetro C (que é o hiper-parametro que restringe o erro máximo aceito para predição) menor será a quantidade de atributos que serão selecionados.\n",
    "\n",
    "Farei um breve experimento do valor mais adequado para o hiper-parametro combinando diferentes valores de C para cada um dos classificadores experimentados. Nomeadamente, vamos experimentar usando C = [0.0001,0.0005, 0.001, 0.005, 0.01]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "selectors = [ \n",
    "    ('lSVC_0.0001', LinearSVC(C=0.0001, penalty=\"l1\", dual=False)),\n",
    "    ('lSVC_0.0005', LinearSVC(C=0.0005, penalty=\"l1\", dual=False)),\n",
    "    ('lSVC_0.001', LinearSVC(C=0.001, penalty=\"l1\", dual=False)),\n",
    "    ('lSVC_0.005', LinearSVC(C=0.005, penalty=\"l1\", dual=False)),\n",
    "    ('lSVC_0.01', LinearSVC(C=0.01, penalty=\"l1\", dual=False))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over Sampling\n",
    "\n",
    "É sabido que existe discrepância entre a quantidade de classes positivas e negativas na coleção. Assim, vou usar uma técnica denominada over-sampling para igualar a quantidade de instancias de cada classe. O objetivo é dar uma clara e variável demonstração para o modelo do que seriam instâncias positivas e negativas para cada um dos problemas.\n",
    "\n",
    "A tecnica de over-sampling que iremos aplicar é denominada SMOTE (Synthetic Minority Over-sampling Technique) e consiste de identificar grupos de instancias similares e replicar seus atributos usando um regressor fraco. Uma implementação desta técnica pode ser encontrada [aqui](http://contrib.scikit-learn.org/imbalanced-learn/stable/index.html). Tal implementação já leva em consideração os dados em formato DataFrame, contudo é preciso usar uma versão diferente de Pipeline para que seja viável seu uso.\n",
    "\n",
    "Assim como as outras tecnicas estatísticas, irei fixar o estado de inicialização para tornar os resultados replicáveis (e novamente será escolhido 42 como valor para tal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline \n",
    "smote = SMOTE(random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação\n",
    "\n",
    "Agora que a coleção de dados já foi proriamente preprocessada, partimos para a classificação das instâncias. Cada uma das 3 abordagens usadas para a tarefa carregam características peculiares. Por exemplo, o Randomforest, que é um dos estados-da-arte para diferentes soluções de classificação, classifica os dados criando um conjunto de árvores de decisão e predizendo os valores de teste com base na combinação do resultado dessas árvores.\n",
    "\n",
    "Cada uma das árvores seleciona, aleatoriamente, uma quantidade de atributos e os ordena por aqueles que são mais discrepantes para predição na tarefa. A combinação dessas diferentes arvores que selecionam os atributos aleatoriamente tende resultar em boas predições, pois diminui a variancia final (que justificaria o overfitting) e tende a dar menos prioridade para atributos menos relevantes.\n",
    "\n",
    "Assim como o RandomForest, o AdaBoost combina o resultado de vários classificadores em uma predição final mais otimizada. Esse tipo de abordagem, denominada ensemble, pode ser usada em diferentes contextos e, em geral, apresenta bons resultados.\n",
    "\n",
    "Diferentemente do RandomForest, esse classificador não foca em atributos mais ou menos relevantes, mas sim em estimar vários classificadores para as instâncias mais \"obvias\" de se predizer. Assim, ele faz cópias desses sub-classificadores e alteram os pesos para que esses novos classificadores fiquem mais propícios às instâncias mais \"dificeis\" de predizer.\n",
    "\n",
    "Em outra via, encontra-se o Multi-Label Perceptron Classifier (MLPClassifier), que é uma técnica baseada em Redes Neuronais. Redes Nuronais tem sido mais frequentemente usadas recentemente, principalmente devido às otimizações que esses algoritmos têm recebido para placas gráficas, permintido que abordagens mais sofisticadas de Redes Neuronais estimem, em tempo viável, modelos mais complexos de predição. Redes Neuronais são recorrentemente usadas como analogia para o cérebro humano, contudo, há demonstrações de que esse tipo de algoritmo de aprendizagem de máquina está mais próximo de algoritmos de otimização não-lineares (como otimização convexa) do que de uma reprensentação digital do cérebro humano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "classifiers = [\n",
    "    ('AdaBoostClassifier', AdaBoostClassifier()),\n",
    "    ('RandomForest_300', RandomForestClassifier(random_state=42, n_estimators=300, max_depth=2)),\n",
    "    ('MLPClassifier', MLPClassifier(alpha=1))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avaliação\n",
    "\n",
    "Agora que todas as sub-etapas da criação dessa solução foram introduzidas, irei apresentar o processo de experimentação para a seleção do melhor modelo. Não levo em consideração abordagens paralelas e nem faço uso de funções mais escaláveis (como a GridSearchCV, que faz uma combinação todos-para-todos dos parâmetros usando validação cruzada) por didática da solução.\n",
    "\n",
    "Assim, farei uso exclusivamente da implementação de Pipeline do pacote imbalanced-learn, variando manualmente as configurações experimentadas.\n",
    "\n",
    "Ao final, apresento um result do AUC (Area Under Curve) do modelo, considerando a interpolação do ROC (Receiver Operating Characteristic) dos resultados esperados para o teste.\n",
    "\n",
    "O ROC é a curva gerada pelos eixos True Positive Rate (TPR) e False Positive Rate (FPR), que nada mais são do que a porcentagem de vezes que o algoritmo identifica as instâncias como positivas quando elas são realmente positivas e a porcentagem de vezes que o algoritmo identifica as instâncias como negativas quando elas são propriamente negativas, respectivamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_upsell 0.767 y_appe 0.772 y_churn 0.771 y_upsell 0.776 y_appe 0.784 y_churn 0.781 y_upsell 0.606 y_appe 0.597 y_churn 0.604 y_upsell 0.828 y_appe 0.829 y_churn 0.829 y_upsell 0.833 y_appe 0.840 y_churn 0.844 y_upsell 0.707 y_appe 0.666 y_churn 0.677 y_upsell 0.834 y_appe 0.835 y_churn 0.835 y_upsell 0.843 y_appe 0.842 y_churn 0.834 y_upsell 0.706 y_appe 0.709 y_churn 0.698 y_upsell 0.835 y_appe 0.836 y_churn 0.839 y_upsell 0.842 y_appe 0.841 y_churn 0.840 y_upsell 0.703 y_appe 0.703 y_churn 0.691 y_upsell 0.834 y_appe 0.831 y_churn 0.837 y_upsell 0.839 y_appe 0.837 y_churn 0.838 y_upsell 0.708 y_appe 0.705 y_churn 0.687 "
     ]
    }
   ],
   "source": [
    "resultados = {}\n",
    "for selector_name, selector in selectors:\n",
    "    for model_name, model in classifiers:\n",
    "        clf = Pipeline([\n",
    "            ('label_encoder', multi_encoder),\n",
    "            ('imputer', imputer),\n",
    "            ('min_max_scaler', minmax_scalar),\n",
    "            ('robust_scaler', robust_scalar),\n",
    "            (selector_name, SelectFromModel(selector)),\n",
    "            ('smote', smote),\n",
    "            (model_name, model)\n",
    "        ])\n",
    "        resultados['%s_%s' % (selector_name, model_name)] = []\n",
    "        for y_name, y in [('y_upsell',y_upsell), ('y_appe',y_appe), ('y_churn',y_churn)]:\n",
    "            result = cross_val_score(clf, X, y_upsell, cv=cv.split(X, y) , scoring='roc_auc')\n",
    "            resultados['%s_%s' % (selector_name, model_name)].extend(result)\n",
    "            print(y_name, '%.3f' % np.mean(result), end=' ')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assim, o modelo ideal para essa coleção de dados (considerando que a combinação de experimentos realizada foi estritamente limitada) é, na ordem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lSVC_0.005_RandomForest_300 0.8408107898866561\n",
      "lSVC_0.001_RandomForest_300 0.8396373569815203\n",
      "lSVC_0.0005_RandomForest_300 0.838726937574723\n",
      "lSVC_0.01_RandomForest_300 0.8379140632899508\n",
      "lSVC_0.005_AdaBoostClassifier 0.8365783129667278\n",
      "lSVC_0.001_AdaBoostClassifier 0.834543219073298\n",
      "lSVC_0.01_AdaBoostClassifier 0.8341265030093582\n",
      "lSVC_0.0005_AdaBoostClassifier 0.8288009105657945\n",
      "lSVC_0.0001_RandomForest_300 0.7805875325315382\n",
      "lSVC_0.0001_AdaBoostClassifier 0.7701211017822427\n",
      "lSVC_0.001_MLPClassifier 0.7045716626251111\n",
      "lSVC_0.01_MLPClassifier 0.6998791857638419\n",
      "lSVC_0.005_MLPClassifier 0.6991275649053992\n",
      "lSVC_0.0005_MLPClassifier 0.6835930391700441\n",
      "lSVC_0.0001_MLPClassifier 0.6022101025637225\n"
     ]
    }
   ],
   "source": [
    "for config, results in sorted(resultados.items(), key=lambda x: np.mean(x[1]), reverse=True):\n",
    "    print(config, np.mean(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dessa forma, o resultado obtido pelo modelo apresentado aqui (0.8408) está relativamente próximo do melhor resultado para essa tarefa (IBM Research: 0.8493). Tais resultados do KDD Cup 2009 podem ser encontrados [aqui](http://www.vincentlemaire-labs.fr/kddcup2009/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
